# ChatFrame

> A cross-platform desktop chatbot that unifies access to multiple LLM providers, supports MCP (Model Context Protocol) servers, and provides built-in retrieval-augmented generation (RAG) for local files. Available for macOS (Apple Silicon & Intel) and Windows (x86_64).

## Overview
ChatFrame delivers a single, polished interface for interacting with language models while giving users full control over their data. A plug-in system for custom tools via MCP and out-of-the-box RAG let you turn any PDF, text, or code file into searchable context—without uploading data to third-party services.

## Quick Start
1. Download the latest release for your OS from [chatframe.co](https://chatframe.co).
2. Launch the app and open **Providers** to add your API keys. You can validate your configuration by clicking **Verify**.
3. Click the first **Chat** button one the left to start
## Supported LLM Providers
- DeepSeek
- OpenAI
- Anthropic
- xAI
- OpenRouter
- Groq
- Moonshot
- Cerebras
- Qwen
- GoogleAIStudio
- Zhipu
- Custom Providers(OpenAI compatible)


## MCP (Model Context Protocol) Support
ChatFrame supports SSE, Streamable HTTP, and Stdio MCP servers, similar to [Cursor](https://cursor.com/).
Add any MCP server to expose its tools at runtime.

### Runtime Environment(STDIO)
Stdio MCP servers require a local runtime environment. For example, to use the Postgres MCP server,

```json
{
  "postgres": {
    "command": "npx",
    "args": [
      "-y",
      "@modelcontextprotocol/server-postgres",
      "postgresql://localhost/mydb"
    ]
  }
}
```

**Node.js must be installed**.

*Next Step*

DXT (one-click MCP installation, similar to browser extensions) will be introduced in the next major release.

*Why aren’t Node.js and Python bundled?*
I'd prefer to let users control their own runtime environments. Bundled interpreters can introduce version conflicts and increase the application footprint.

## Projects
Create workspaces, upload local files (PDF, TXT, MD, code), and instantly build vector indexes for retrieval-augmented generation.

## Chat
- Invoke any MCP tool from within a conversation
- Multimodal input (images and text files)
- Live **artifacts** – render React components, HTML, SVG, or code snippets in an isolated sandbox that updates in real time

## Pricing
ChatFrame is currently free in limited features, unlock pro features to get full features


## Feature Map
- **Chat**
  - Text chat
  - Model selection
  - Tool selection via MCP servers
  - Artifacts: create interactive content generated by the LLM
  - Attachments: upload images, PDFs, and text files (PDFs are parsed locally)

- **Projects**
    - Create and then manage projects
    - Start a chat from a project
- **MCP Servers**
    - Install any kind of MCP server
    - Run the server by clicking start
- **Model Providers**
    - *
- **Settings**
  - **App**
    - Appearance: System / Light / Dark
    - Updates: ChatFrame downloads updates in the background and displays an install button when ready
  - **Shortcuts**
    - New Chat: ⌘N
    - Toggle Sidebar: ⌘B
  - **Advanced**
    - Proxy URL: sets the `all_proxy` environment variable. Leave blank to disable proxying. When configured, all LLM API requests are routed through the specified proxy.
